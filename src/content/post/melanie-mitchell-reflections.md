---
title: "AI: A Guide for Thinking Humans ‚Äî Reflections"
description: "Two months with Melanie Mitchell‚Äôs book, read in 2025, six years after its publication. Between history, convolutional networks, AlphaGo, Winograd schemas, and the eternal AI effect."
publishDate: "2025-09-15T08:00:00Z"
tags: ["Melanie Mitchell", "AI", "books", "reflection", "history of AI", "AlphaGo", "Winograd"]
hidden: false
---

# ü§ñüìö Reflections on Melanie Mitchell ‚Äî *Artificial Intelligence: A Guide for Thinking Humans*

I‚Äôve finished Melanie Mitchell‚Äôs book, written back in 2019. At times I was fascinated, at times irritated, but never indifferent. It is not a trivial book ‚Äî and reading it in 2025 gives it a very different flavor.  

Even among the brightest minds today, there are still two opposing views: either AI will be transformative beyond imagination, or it will remain brittle statistical tricks. After two months of learning and six years of hindsight since Mitchell‚Äôs publication, I land here: she was right about one thing ‚Äî **ASI is still far away**. But when it comes to **ANI** and even emerging **AGI-like behaviors**, she underestimated what brilliant engineers could achieve.  

Her flaw, in my view, was dwelling too much on imperfections. It‚Äôs like criticizing an extraordinary program for missing one feature, instead of seeing how much has been accomplished and how close the rest lies within reach. Measuring AI only by what it *cannot yet do* is a skewed lens.

---

## Part One: *Background*

This was Mitchell at her best. She maps the full arc of AI history: from Dartmouth 1956, through the AI winters, up to the deep learning boom fueled by data and compute.  

It calmed my own fears about **ASI** ‚Äî no, it is not lurking just around the corner, and here she aligns with Kai-Fu Lee and most sober voices (Kurzweil being the exception).  

Most importantly, she reminded me that AI did not *begin* in 2023 with ChatGPT. It is a decades-long endeavor, waxing and waning, now amplified by GPUs and massive datasets. That perspective is invaluable.

---

## Part Two: *Looking and Seeing*

This section pushed me the hardest, and gave the most technical substance. Neural networks inspired by neuroscience, feed-forward layers, the virtual cortex analogy: light hitting the retina, activating cones and rods, passing through the optic nerve, triggering neurons in the occipital cortex layer by layer.  

Terms like **neurons, dendrites, axons, soma, synapses** appear alongside **convolution, receptive field, activation map vs. feature map**.  

I had to take breaks, dive into other sources, even wrestle with CNNs in detail to fully understand convolution. It was the most demanding part of the book ‚Äî but also the richest in learning.

---

## Philosophy and Skepticism

From there the tone shifted. She began pushing back against Kurzweil‚Äôs extremes, and more broadly against the idea that AI could ever count as *intelligence*. Her skepticism grew chapter by chapter.  

Amusing side note I learned: Alan Turing himself, despite his genius, believed in hypnosis. Even the brightest minds can be naive ‚Äî like the decades when ulcers were blamed on stress before Marshall proved otherwise.  

On the **Turing Test**, Mitchell insisted machines still cannot pass. And when something *did* seem to pass in the 2010s, she brushed it aside: *not the real thing*.  

At least she reminded us of John McCarthy‚Äôs timeless quip: *‚ÄúAs soon as it works, no one calls it AI anymore.‚Äù* So true.  

History also shows why we shouldn‚Äôt fully trust expert predictions: in the 60s, 70s, 80s, many brilliant minds said AGI was just a few years away. They were wrong then, but the lesson cuts both ways.

---

## Part Three: *Learning to Play*

Here, for me, Mitchell failed to give proper credit.  
- **DeepBlue vs. Kasparov** ‚Äî dismissed as brute force, symbolic AI, not ‚Äúreal intelligence.‚Äù  
- **Watson in Jeopardy!** ‚Äî dismissed again, not AI.  
- **AlphaGo and AlphaZero** ‚Äî astonishing achievements reduced to ‚Äújust games.‚Äù  

But Go is *more complex* than most things humans do daily. If machines can conquer such domains, what about simpler tasks? The question is rhetorical.  

Once again, McCarthy‚Äôs line echoes: as soon as it works, we rename it brute force, statistics, not AI.

---

## Computer Vision and *Clever Hans*

Her critique of CV was familiar: **ImageNet 2012, AlexNet, long tail edge cases, overfitting, adversarial blur turning a cat into ‚Äúanimal.‚Äù**  

To be fair, she also highlighted a deeper **methodological issue**. Benchmarks often boast ‚Äúsuperhuman‚Äù results, but usually in *top-5 accuracy* across narrowly defined categories. In *top-1 accuracy* models were still far from flawless.  

She cited Andrej Karpathy (today better known for leading Tesla‚Äôs AI team), who once showed a photo of Obama with several officials. Obama is lightly stepping on a scale one of them is standing on ‚Äî a playful, subtle joke. Humans instantly grasp the humor from sparse visual cues and shared context. A model, with only pixels and no background knowledge, has no chance.  

It‚Äôs a fair point: humans interpret images not just with eyes but with world knowledge, culture, and embodied common sense. Models, in 2019, did not.

---

## Part Four: *AI Meets NLP*

This is where her book aged the worst. She leaned heavily on ambiguous stories and **Winograd schemas**, saying NLP could never solve them because they required *commonsense reasoning*.  

Six years later? GPT-5 solves them effortlessly.  

She invoked the engineering adage: *the last 10% takes 90% of the time*. Perhaps true in some fields ‚Äî but here the curve was steeper. The last 10% fell much faster than expected.  

---

## Part Five: *The Barrier of Meaning*

By this point, her argument lost momentum. She tried to gather predictions, to stand as the voice of cautious realism. But after the collapse of her NLP critique, it felt hollow.  

For me, the uniqueness of human reasoning and creativity looks less mystical now. Reading Greene‚Äôs *Mastery* in parallel, I saw that so-called ‚Äúmiraculous‚Äù creations often came from **unique mixes of prior experience**, later combined in novel ways. That is not unlike what diffusion models do today ‚Äî remixing latent representations into surprising new outputs.  

When GPT writes an essay in the style of a famous philosopher, it no longer feels like a cheap trick. It feels like exactly the kind of remix humans have always done.

---

## Conclusion

Mitchell‚Äôs book remains valuable as **history and introduction**. She is right: ASI is not imminent.  

But her relentless skepticism ‚Äî especially toward landmark achievements like DeepBlue, Watson, AlphaGo, and NLP ‚Äî blinded her to the magnitude of what had been done. She focused on missing features instead of acknowledging the scope of the breakthroughs.  

In 2025, her work reads more like a **document of the pre-ChatGPT era** than a guide for today.  

And ironically, she fell into the very trap she quoted from McCarthy: *‚ÄúAs soon as it works, no one calls it AI anymore.‚Äù*  
That line has never felt more true.  

---

